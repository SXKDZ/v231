---
abstract: Despite being the go-to choice for link prediction on knowledge graphs,
  research on interpretability of knowledge graph embeddings (KGE) has been relatively
  unexplored. We present KGEx, a novel post-hoc method that explains individual link
  predictions by drawing inspiration from surrogate models research. Given a target
  triple to predict, KGEx trains surrogate KGE models that we use to identify important
  training triples. To gauge the impact of a training triple, we sample random portions
  of the target triple neighborhood and we train multiple surrogate KGE models on
  each of them. To ensure faithfulness, each surrogate is trained by distilling knowledge
  from the original KGE model. We then assess how well surrogates predict the target
  triple being explained, the intuition being that those leading to faithful predictions
  have been trained on \textasciigrave \textasciigrave impactful‚Äùneighborhood samples.
  Under this assumption, we then harvest triples that appear frequently across impactful
  neighborhoods.  We conduct extensive experiments on two publicly available datasets,
  to demonstrate that KGEx is capable of providing explanations faithful to the black-box
  model.
openreview: NSXXSyc2DF
section: Poster Presentations
software: https://github.com/Accenture/AmpliGraph
title: 'KGEx: Explaining Knowledge Graph Embeddings via Subgraph Sampling and Knowledge
  Distillation'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: baltatzis24a
month: 0
tex_title: 'KGEx: Explaining Knowledge Graph Embeddings via Subgraph Sampling and
  Knowledge Distillation'
firstpage: '27:1'
lastpage: '27:13'
page: 27:1-27:13
order: 27
cycles: false
bibtex_author: Baltatzis, Vasileios and Costabello, Luca
author:
- given: Vasileios
  family: Baltatzis
- given: Luca
  family: Costabello
date: 2024-04-17
address:
container-title: Proceedings of the Second Learning on Graphs Conference
volume: '231'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 17
pdf: https://proceedings.mlr.press/v231/baltatzis24a/baltatzis24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
