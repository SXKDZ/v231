---
abstract: 'Learning models that execute algorithms can enable us to address a key
  problem in deep learning: generalizing to out-of-distribution data. However, neural
  networks are currently unable to execute recursive algorithms because they do not
  have arbitrarily large memory to store and recall state. To address this, we (1)
  propose a way to augment graph neural networks (GNNs) with a stack, and (2) develop
  an approach for sampling intermediate algorithm trajectories that improves alignment
  with recursive algorithms over previous methods. The stack allows the network to
  learn to store and recall a portion of the state of the network at a particular
  time, analogous to the action of a call stack in a recursive algorithm. This augmentation
  permits the network to reason recursively. We empirically demonstrate that our proposals
  significantly improve generalization to larger input graphs over prior work on depth-first
  search (DFS).'
openreview: 43M1bPorxU
section: Oral Presentations
software: https://github.com/DJayalath/gnn-call-stack/
title: Recursive Algorithmic Reasoning
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jurss24a
month: 0
tex_title: Recursive Algorithmic Reasoning
firstpage: '5:1'
lastpage: '5:14'
page: 5:1-5:14
order: 5
cycles: false
bibtex_author: J{\"u}r{\ss}, Jonas and Jayalath, Dulhan Hansaja and Veli{\v c}kovi{\'
  c}, Petar
author:
- given: Jonas
  family: Jürß
- given: Dulhan Hansaja
  family: Jayalath
- given: Petar
  family: Veličković
date: 2024-04-17
address:
container-title: Proceedings of the Second Learning on Graphs Conference
volume: '231'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 17
pdf: https://proceedings.mlr.press/v231/jurss24a/jurss24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
