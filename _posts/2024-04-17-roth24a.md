---
abstract: Our study reveals new theoretical insights into over-smoothing and feature
  over-correlation in graph neural networks. Specifically, we demonstrate that with
  increased depth, node representations become dominated by a low-dimensional subspace
  that depends on the aggregation function but not on the feature transformations.
  For all aggregation functions, the rank of the node representations collapses, resulting
  in over-smoothing for particular aggregation functions. Our study emphasizes the
  importance for future research to focus on rank collapse rather than over-smoothing.
  Guided by our theory, we propose a sum of Kronecker products as a beneficial property
  that provably prevents over-smoothing, over-correlation, and rank collapse. We empirically
  demonstrate the shortcomings of existing models in fitting target functions of node
  classification tasks.
openreview: 9aIDdGm7a6
section: Poster Presentations
title: Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: roth24a
month: 0
tex_title: Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural
  Networks
firstpage: '35:1'
lastpage: '35:23'
page: 35:1-35:23
order: 35
cycles: false
bibtex_author: Roth, Andreas and Liebig, Thomas
author:
- given: Andreas
  family: Roth
- given: Thomas
  family: Liebig
date: 2024-04-17
address:
container-title: Proceedings of the Second Learning on Graphs Conference
volume: '231'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 17
pdf: https://proceedings.mlr.press/v231/roth24a/roth24a.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v231/roth24a/roth24a-supp.zip
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
